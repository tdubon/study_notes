{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3aa912",
   "metadata": {},
   "source": [
    "## Topic Modeling: Comparing LDA & BerTopic Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655df979",
   "metadata": {},
   "source": [
    "#### LDA with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d70eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b9714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5eedf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/tdubon/Documents/Plato_republic_sample.txt', \"r\") as file:\n",
    "    lines = [line.strip().lower() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e90b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "\n",
    "for i in lines:\n",
    "    doc = nlp(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c456f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for which i am indebted to you, i said, now that you have grown gentle towards me and have left off scolding. nevertheless, i have not been well entertained; but that was my own fault and not yours. as an epicure snatches a taste of every dish which is successively brought to table, he not having allowed himself time to enjoy the one before, so have i gone from one subject to another without having discovered what i sought at first, the nature of justice. i left that enquiry and turned away to consider whether justice is virtue and wisdom or evil and folly; and when there arose a further question about the comparative advantages of justice and injustice, i could not refrain from passing on to that. and the result of the whole discussion has been that i know nothing at all. for i know not what justice is, and therefore i am not likely to know whether it is or is not a virtue, nor can i say whether the just man is happy or unhappy.\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18de10a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for\n",
      "which\n",
      "i\n",
      "am\n",
      "indebted\n",
      "to\n",
      "you\n",
      ",\n",
      "i\n",
      "said\n",
      ",\n",
      "now\n",
      "that\n",
      "you\n",
      "have\n",
      "grown\n",
      "gentle\n",
      "towards\n",
      "me\n",
      "and\n",
      "have\n",
      "left\n",
      "off\n",
      "scolding\n",
      ".\n",
      "nevertheless\n",
      ",\n",
      "i\n",
      "have\n",
      "not\n",
      "been\n",
      "well\n",
      "entertained\n",
      ";\n",
      "but\n",
      "that\n",
      "was\n",
      "my\n",
      "own\n",
      "fault\n",
      "and\n",
      "not\n",
      "yours\n",
      ".\n",
      "as\n",
      "an\n",
      "epicure\n",
      "snatches\n",
      "a\n",
      "taste\n",
      "of\n",
      "every\n",
      "dish\n",
      "which\n",
      "is\n",
      "successively\n",
      "brought\n",
      "to\n",
      "table\n",
      ",\n",
      "he\n",
      "not\n",
      "having\n",
      "allowed\n",
      "himself\n",
      "time\n",
      "to\n",
      "enjoy\n",
      "the\n",
      "one\n",
      "before\n",
      ",\n",
      "so\n",
      "have\n",
      "i\n",
      "gone\n",
      "from\n",
      "one\n",
      "subject\n",
      "to\n",
      "another\n",
      "without\n",
      "having\n",
      "discovered\n",
      "what\n",
      "i\n",
      "sought\n",
      "at\n",
      "first\n",
      ",\n",
      "the\n",
      "nature\n",
      "of\n",
      "justice\n",
      ".\n",
      "i\n",
      "left\n",
      "that\n",
      "enquiry\n",
      "and\n",
      "turned\n",
      "away\n",
      "to\n",
      "consider\n",
      "whether\n",
      "justice\n",
      "is\n",
      "virtue\n",
      "and\n",
      "wisdom\n",
      "or\n",
      "evil\n",
      "and\n",
      "folly\n",
      ";\n",
      "and\n",
      "when\n",
      "there\n",
      "arose\n",
      "a\n",
      "further\n",
      "question\n",
      "about\n",
      "the\n",
      "comparative\n",
      "advantages\n",
      "of\n",
      "justice\n",
      "and\n",
      "injustice\n",
      ",\n",
      "i\n",
      "could\n",
      "not\n",
      "refrain\n",
      "from\n",
      "passing\n",
      "on\n",
      "to\n",
      "that\n",
      ".\n",
      "and\n",
      "the\n",
      "result\n",
      "of\n",
      "the\n",
      "whole\n",
      "discussion\n",
      "has\n",
      "been\n",
      "that\n",
      "i\n",
      "know\n",
      "nothing\n",
      "at\n",
      "all\n",
      ".\n",
      "for\n",
      "i\n",
      "know\n",
      "not\n",
      "what\n",
      "justice\n",
      "is\n",
      ",\n",
      "and\n",
      "therefore\n",
      "i\n",
      "am\n",
      "not\n",
      "likely\n",
      "to\n",
      "know\n",
      "whether\n",
      "it\n",
      "is\n",
      "or\n",
      "is\n",
      "not\n",
      "a\n",
      "virtue\n",
      ",\n",
      "nor\n",
      "can\n",
      "i\n",
      "say\n",
      "whether\n",
      "the\n",
      "just\n",
      "man\n",
      "is\n",
      "happy\n",
      "or\n",
      "unhappy\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b73cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words and punctuation\n",
    "stop_words = []\n",
    "\n",
    "def remove_words(doc):\n",
    "    for token in doc:\n",
    "        if token.is_punct == True:\n",
    "            stop_words.append(token)\n",
    "        \n",
    "        if token.is_stop == True:\n",
    "            stop_words.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d682d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2300788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[for, which, i, am, to, you, ,, i, ,, now, that, you, have, towards, me, and, have, off, ., nevertheless, ,, i, have, not, been, well, ;, but, that, was, my, own, and, not, yours, ., as, an, a, of, every, which, is, to, ,, he, not, himself, to, the, one, before, ,, so, have, i, from, one, to, another, without, what, i, at, first, ,, the, of, ., i, that, and, to, whether, is, and, or, and, ;, and, when, there, a, further, about, the, of, and, ,, i, could, not, from, on, to, that, ., and, the, of, the, whole, has, been, that, i, nothing, at, all, ., for, i, not, what, is, ,, and, therefore, i, am, not, to, whether, it, is, or, is, not, a, ,, nor, can, i, say, whether, the, just, is, or, .]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efbd3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[for, which, i, am, to, you, ,, i, ,, now, that, you, have, towards, me, and, have, off, ., nevertheless, ,, i, have, not, been, well, ;, but, that, was, my, own, and, not, yours, ., as, an, a, of, every, which, is, to, ,, he, not, himself, to, the, one, before, ,, so, have, i, from, one, to, another, without, what, i, at, first, ,, the, of, ., i, that, and, to, whether, is, and, or, and, ;, and, when, there, a, further, about, the, of, and, ,, i, could, not, from, on, to, that, ., and, the, of, the, whole, has, been, that, i, nothing, at, all, ., for, i, not, what, is, ,, and, therefore, i, am, not, to, whether, it, is, or, is, not, a, ,, nor, can, i, say, whether, the, just, is, or, .]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c901371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of words\n",
    "doc_list = []\n",
    "\n",
    "for token in doc:\n",
    "    if token not in stop_words:\n",
    "        doc_list.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94818f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['indebted', 'said', 'grown', 'gentle', 'left', 'scolding', 'entertained', 'fault', 'epicure', 'snatches', 'taste', 'dish', 'successively', 'brought', 'table', 'having', 'allowed', 'time', 'enjoy', 'gone', 'subject', 'having', 'discovered', 'sought', 'nature', 'justice', 'left', 'enquiry', 'turned', 'away', 'consider', 'justice', 'virtue', 'wisdom', 'evil', 'folly', 'arose', 'question', 'comparative', 'advantages', 'justice', 'injustice', 'refrain', 'passing', 'result', 'discussion', 'know', 'know', 'justice', 'likely', 'know', 'virtue', 'man', 'happy', 'unhappy']\n"
     ]
    }
   ],
   "source": [
    "print(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f5d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates inputs to topic model -  which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary([doc_list]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f51415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9add64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4023541a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.021*\"justice\" + 0.021*\"know\" + 0.021*\"left\" + 0.021*\"said\" + '\n",
      "  '0.021*\"likely\" + 0.021*\"man\" + 0.021*\"nature\" + 0.021*\"passing\" + '\n",
      "  '0.021*\"refrain\" + 0.021*\"result\"'),\n",
      " (1,\n",
      "  '0.021*\"virtue\" + 0.021*\"know\" + 0.021*\"justice\" + 0.021*\"passing\" + '\n",
      "  '0.021*\"likely\" + 0.021*\"result\" + 0.021*\"scolding\" + 0.021*\"nature\" + '\n",
      "  '0.021*\"left\" + 0.021*\"man\"'),\n",
      " (2,\n",
      "  '0.021*\"justice\" + 0.021*\"know\" + 0.021*\"refrain\" + 0.021*\"left\" + '\n",
      "  '0.021*\"likely\" + 0.021*\"man\" + 0.021*\"nature\" + 0.021*\"passing\" + '\n",
      "  '0.021*\"question\" + 0.021*\"indebted\"'),\n",
      " (3,\n",
      "  '0.021*\"virtue\" + 0.021*\"justice\" + 0.021*\"nature\" + 0.021*\"likely\" + '\n",
      "  '0.021*\"question\" + 0.021*\"passing\" + 0.021*\"result\" + 0.021*\"left\" + '\n",
      "  '0.021*\"man\" + 0.021*\"know\"'),\n",
      " (4,\n",
      "  '0.022*\"justice\" + 0.022*\"know\" + 0.021*\"virtue\" + 0.021*\"left\" + '\n",
      "  '0.021*\"having\" + 0.021*\"passing\" + 0.021*\"epicure\" + 0.021*\"scolding\" + '\n",
      "  '0.021*\"refrain\" + 0.021*\"enjoy\"'),\n",
      " (5,\n",
      "  '0.069*\"justice\" + 0.052*\"know\" + 0.035*\"left\" + 0.035*\"having\" + '\n",
      "  '0.035*\"virtue\" + 0.018*\"enjoy\" + 0.018*\"man\" + 0.018*\"subject\" + '\n",
      "  '0.018*\"turned\" + 0.018*\"passing\"'),\n",
      " (6,\n",
      "  '0.021*\"justice\" + 0.021*\"know\" + 0.021*\"said\" + 0.021*\"left\" + '\n",
      "  '0.021*\"likely\" + 0.021*\"man\" + 0.021*\"nature\" + 0.021*\"passing\" + '\n",
      "  '0.021*\"refrain\" + 0.021*\"result\"'),\n",
      " (7,\n",
      "  '0.023*\"justice\" + 0.023*\"know\" + 0.022*\"virtue\" + 0.022*\"having\" + '\n",
      "  '0.022*\"left\" + 0.021*\"refrain\" + 0.021*\"grown\" + 0.021*\"said\" + '\n",
      "  '0.021*\"successively\" + 0.021*\"gentle\"'),\n",
      " (8,\n",
      "  '0.021*\"justice\" + 0.021*\"know\" + 0.021*\"nature\" + 0.021*\"man\" + '\n",
      "  '0.021*\"question\" + 0.021*\"passing\" + 0.021*\"result\" + 0.021*\"likely\" + '\n",
      "  '0.021*\"snatches\" + 0.021*\"left\"'),\n",
      " (9,\n",
      "  '0.021*\"know\" + 0.021*\"justice\" + 0.021*\"having\" + 0.021*\"man\" + '\n",
      "  '0.021*\"left\" + 0.021*\"nature\" + 0.021*\"question\" + 0.021*\"likely\" + '\n",
      "  '0.021*\"virtue\" + 0.021*\"result\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cfedf",
   "metadata": {},
   "source": [
    "#### Bertopic with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdd9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee24c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb1a5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20e55457",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/tdubon/Documents/Plato_republic_sample.txt', \"r\") as file:\n",
    "    lines = [line.strip().lower() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5feac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "\n",
    "for i in lines:\n",
    "    doc = nlp(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "975ad6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words and punctuation\n",
    "stop_words = []\n",
    "\n",
    "def remove_words(doc):\n",
    "    for token in doc:\n",
    "        if token.is_punct == True:\n",
    "            stop_words.append(token)\n",
    "        \n",
    "        if token.is_stop == True:\n",
    "            stop_words.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0fcfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a201d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of words\n",
    "doc_list = []\n",
    "\n",
    "for token in doc:\n",
    "    if token not in stop_words:\n",
    "        doc_list.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2efcd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTopic(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdb33de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efd4b6fed644ae8bbb3c7f3b0c4424a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 19:03:36,685 - BERTopic - Transformed documents to Embeddings\n",
      "OMP: Info #274: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2022-03-04 19:03:46,964 - BERTopic - Reduced dimensionality with UMAP\n",
      "2022-03-04 19:03:47,010 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    }
   ],
   "source": [
    "topic, probabilitites = model.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2583da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>55</td>\n",
       "      <td>-1_justice_know_virtue_having</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                           Name\n",
       "0     -1     55  -1_justice_know_virtue_having"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_info()                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "267cac34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('justice', 0.1957267696571512),\n",
       " ('know', 0.16155440301154414),\n",
       " ('virtue', 0.1218146940827129),\n",
       " ('having', 0.1218146940827129),\n",
       " ('left', 0.1218146940827129),\n",
       " ('', 1e-05),\n",
       " ('unhappy', 0.0731882125588209),\n",
       " ('consider', 0.0731882125588209),\n",
       " ('discussion', 0.0731882125588209),\n",
       " ('discovered', 0.0731882125588209)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00926e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
